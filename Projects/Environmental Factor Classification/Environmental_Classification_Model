{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Unzip the folder 'Project1'**"
      ],
      "metadata": {
        "id": "kkHWWtB5hUUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Create 16 Statistical Feature Dataset**"
      ],
      "metadata": {
        "id": "5o3FVu96hs4_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD6gIe2ihTkC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.linalg import eigh\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def process_file(file_path):\n",
        "    # Extract class information from the filename\n",
        "    file_name = os.path.basename(file_path)\n",
        "    class_info = file_name.split('_')[3]\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # 1. Count the occurrences of each StopID for each Frame Num\n",
        "    frame_stop_counts = df.groupby(['Frame Num', 'StopID']).size().unstack(fill_value=0)\n",
        "    frame_stop_counts.columns = [f\"{col}_stop_counts\" for col in frame_stop_counts.columns]\n",
        "\n",
        "    # 2. Calculate the mean distance for each StopID for each Frame Num\n",
        "    stop_distance_means = df.groupby(['Frame Num', 'StopID'])['Distance'].mean().unstack(fill_value=0)\n",
        "    stop_distance_means.columns = [f\"{col}_stop_distance_means\" for col in stop_distance_means.columns]\n",
        "\n",
        "    # 3. Calculate the mean and variance of StopID for each Frame Num\n",
        "    stop_mean_variance = df.groupby('Frame Num')['StopID'].agg(['mean', 'var'])\n",
        "    stop_mean_variance.columns = [f\"StopID_{col}\" for col in stop_mean_variance.columns]\n",
        "\n",
        "    # 4. Calculate the mean distance for each Frame Num\n",
        "    distance_means = df.groupby('Frame Num')['Distance'].mean().rename('Distance_mean')\n",
        "\n",
        "    # 5. Calculate the mean Azimuth for each Frame Num\n",
        "    azimuth_means = df.groupby('Frame Num')['Azimuth'].mean().rename('Azimuth_mean')\n",
        "\n",
        "    # 6. Calculate the mean VerticalAngle for each Frame Num\n",
        "    vertical_angle_means = df.groupby('Frame Num')['VerticalAngle'].mean().rename('VerticalAngle_mean')\n",
        "\n",
        "    # 7. Calculate the mean and variance of Intensity for each Frame Num\n",
        "    intensity_mean_variance = df.groupby('Frame Num')['Intensity'].agg(['mean', 'var'])\n",
        "    intensity_mean_variance.columns = [f\"Intensity_{col}\" for col in intensity_mean_variance.columns]\n",
        "\n",
        "    # 8. Calculate the eigenvalues of the covariance matrix for x, y, z for each Frame Num\n",
        "    eig_values = df.groupby('Frame Num')[['X', 'Y', 'Z']].apply(lambda x: pd.Series(eigh(np.cov(x, rowvar=False))[0], index=['eig_cov_x', 'eig_cov_y', 'eig_cov_z']))\n",
        "\n",
        "    # Create a DataFrame with the results\n",
        "    result_df = pd.concat([frame_stop_counts, stop_distance_means, stop_mean_variance, distance_means, azimuth_means, vertical_angle_means, intensity_mean_variance, eig_values], axis=1)\n",
        "\n",
        "    # Fill NaN values with 0\n",
        "    result_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Add class column\n",
        "    result_df['Class'] = class_info\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Path to the input files\n",
        "input_path = 'path/to/your/input/*.csv'  # e.g., 'C:/user/documents/input/*.csv'\n",
        "output_path = 'path/to/your/output/combined_output.csv'  # e.g., 'C:/user/documents/output/combined_output.csv'\n",
        "\n",
        "# Process each file and combine the results\n",
        "all_results = []\n",
        "for file in glob.glob(input_path):\n",
        "    result = process_file(file)\n",
        "    all_results.append(result)\n",
        "\n",
        "# Combine all results into a single DataFrame\n",
        "combined_df = pd.concat(all_results)\n",
        "\n",
        "# Save the result to a CSV file\n",
        "combined_df.to_csv(output_path, index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. K-Nearest Neighbor Model (K=10)**"
      ],
      "metadata": {
        "id": "KD17SIOjjb5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# File Load\n",
        "file_path = 'path/to/your/output/combined_output.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define Feature X and Target Y\n",
        "X = data.drop(['Number', 'Class'], axis=1)\n",
        "y = data['Class']\n",
        "\n",
        "# Divide Train set (80%) and Test set(20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
        "\n",
        "# Data Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape\n",
        "\n",
        "# KNN Model (k=10)\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Accuracy\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "train_accuracy = knn.score(X_train_scaled,y_train)\n",
        "print(f'Train accuracy: {train_accuracy}')\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "CNuzAHsPhre4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Random Forest Model**"
      ],
      "metadata": {
        "id": "XU6pkiTjkcei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# File Load\n",
        "file_path = 'path/to/your/output/combined_output.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(['Number', 'Class'], axis=1)\n",
        "y = data['Class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the decision tree model\n",
        "dt_classifier = DecisionTreeClassifier(max_depth=16, random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model's accuracy on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "train_accuracy = dt_classifier.score(X_train,y_train)\n",
        "print(f'Train accuracy: {train_accuracy}')\n",
        "print(f'Test accuracy: {test_accuracy}')\n",
        "\n",
        "# Plot the tree structure\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plot_tree(dt_classifier, filled=True, feature_names=X.columns, class_names=str(dt_classifier.classes_))\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "pd.Series(dt_classifier.feature_importances_, index=X.columns).sort_values(ascending=False).plot(kind='bar')\n",
        "plt.title('Feature Importances')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bHXYwtgUkwI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Deep Neural Network Model**"
      ],
      "metadata": {
        "id": "EpmySevYkMtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# File Load\n",
        "file_path = 'path/to/your/output/combined_output.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(['Number', 'Class'], axis=1)  # Drop 'Number' and 'Class' columns\n",
        "y = data['Class']  # Use 'Class' column as target\n",
        "\n",
        "# Data scaling\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Split the data into training and testing sets (70% training, 30% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# One-hot encode the target variable\n",
        "num_classes = len(np.unique(y_train))\n",
        "y_train_encoded = to_categorical(y_train - 1, num_classes)\n",
        "y_test_encoded = to_categorical(y_test - 1, num_classes)\n",
        "\n",
        "# Early stopping callback setup\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Build a DNN model with L2 regularization\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.02), input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.02)),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Setup Adam optimizer\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train_encoded,\n",
        "    epochs=50,\n",
        "    batch_size=10,\n",
        "    verbose=1,\n",
        "    validation_split=0.3,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_loss, train_accuracy = model.evaluate(X_train_scaled, y_train_encoded, verbose=0)\n",
        "print(f'Training Accuracy: {train_accuracy}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_encoded, verbose=0)\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "# Visualization of training loss and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QOaLoe9SlCFJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
