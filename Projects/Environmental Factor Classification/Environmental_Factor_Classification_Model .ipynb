{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VlY533DP8eP"
   },
   "source": [
    "## **Before Training Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkHWWtB5hUUX"
   },
   "source": [
    "**1. Unzip the folder 'Project1'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoTDvNeTPQBO"
   },
   "source": [
    "**2. Download Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4laUaPuPVS6"
   },
   "outputs": [],
   "source": [
    "pip install pandas numpy scipy scikit-learn tensorflow keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5o3FVu96hs4_"
   },
   "source": [
    "**3. Create 16 Statistical Feature Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD6gIe2ihTkC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def process_file(file_path):\n",
    "    # Extract class information from the filename\n",
    "    file_name = os.path.basename(file_path)\n",
    "    class_info = file_name.split('_')[3]\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 1. Count the occurrences of each StopID for each Frame Num\n",
    "    frame_stop_counts = df.groupby(['Frame Num', 'StopID']).size().unstack(fill_value=0)\n",
    "    frame_stop_counts.columns = [f\"{col}_stop_counts\" for col in frame_stop_counts.columns]\n",
    "\n",
    "    # 2. Calculate the mean distance for each StopID for each Frame Num\n",
    "    stop_distance_means = df.groupby(['Frame Num', 'StopID'])['Distance'].mean().unstack(fill_value=0)\n",
    "    stop_distance_means.columns = [f\"{col}_stop_distance_means\" for col in stop_distance_means.columns]\n",
    "\n",
    "    # 3. Calculate the mean and variance of StopID for each Frame Num\n",
    "    stop_mean_variance = df.groupby('Frame Num')['StopID'].agg(['mean', 'var'])\n",
    "    stop_mean_variance.columns = [f\"StopID_{col}\" for col in stop_mean_variance.columns]\n",
    "\n",
    "    # 4. Calculate the mean distance for each Frame Num\n",
    "    distance_means = df.groupby('Frame Num')['Distance'].mean().rename('Distance_mean')\n",
    "\n",
    "    # 5. Calculate the mean Azimuth for each Frame Num\n",
    "    azimuth_means = df.groupby('Frame Num')['Azimuth'].mean().rename('Azimuth_mean')\n",
    "\n",
    "    # 6. Calculate the mean VerticalAngle for each Frame Num\n",
    "    vertical_angle_means = df.groupby('Frame Num')['VerticalAngle'].mean().rename('VerticalAngle_mean')\n",
    "\n",
    "    # 7. Calculate the mean and variance of Intensity for each Frame Num\n",
    "    intensity_mean_variance = df.groupby('Frame Num')['Intensity'].agg(['mean', 'var'])\n",
    "    intensity_mean_variance.columns = [f\"Intensity_{col}\" for col in intensity_mean_variance.columns]\n",
    "\n",
    "    # 8. Calculate the eigenvalues of the covariance matrix for x, y, z for each Frame Num\n",
    "    eig_values = df.groupby('Frame Num')[['X', 'Y', 'Z']].apply(lambda x: pd.Series(eigh(np.cov(x, rowvar=False))[0], index=['eig_cov_x', 'eig_cov_y', 'eig_cov_z']))\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    result_df = pd.concat([frame_stop_counts, stop_distance_means, stop_mean_variance, distance_means, azimuth_means, vertical_angle_means, intensity_mean_variance, eig_values], axis=1)\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    result_df.fillna(0, inplace=True)\n",
    "\n",
    "    # Add class column\n",
    "    result_df['Class'] = class_info\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Path to the input files\n",
    "input_path = 'path/to/your/input/*.csv'  # e.g., 'C:/user/documents/input/*.csv'\n",
    "output_path = 'path/to/your/output/combined_output.csv'  # e.g., 'C:/user/documents/output/combined_output.csv'\n",
    "\n",
    "# Process each file and combine the results\n",
    "all_results = []\n",
    "for file in glob.glob(input_path):\n",
    "    result = process_file(file)\n",
    "    all_results.append(result)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "combined_df = pd.concat(all_results)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "combined_df.to_csv(output_path, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBqfT4E3Phvu"
   },
   "source": [
    "# **Training Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD17SIOjjb5j"
   },
   "source": [
    "**K-Nearest Neighbor Model (K=10)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNuzAHsPhre4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# File Load\n",
    "file_path = 'path/to/your/output/combined_output.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define Feature X and Target Y\n",
    "X = data.drop(['Frame Num', 'Class'], axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# Divide Train set (70%) and Test set(30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Data Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape\n",
    "\n",
    "# KNN Model (k=10)\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "train_accuracy = knn.score(X_train_scaled,y_train)\n",
    "print(f'Train accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU6pkiTjkcei"
   },
   "source": [
    "**Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHXYwtgUkwI1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# File Load\n",
    "file_path = 'path/to/your/output/combined_output.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop(['Frame Num', 'Class'], axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the RandomForest model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=16, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's accuracy on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "train_accuracy = rf_classifier.score(X_train, y_train)\n",
    "print(f'Train accuracy: {train_accuracy}')\n",
    "print(f'Test accuracy: {test_accuracy}')\n",
    "\n",
    "# Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "pd.Series(rf_classifier.feature_importances_, index=X.columns).sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpmySevYkMtb"
   },
   "source": [
    "**Deep Neural Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOaLoe9SlCFJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# File Load\n",
    "file_path = '/content/drive/MyDrive/combined output.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop(['Frame Num', 'Class'], axis=1)\n",
    "y = data['Class']  # Use 'Class' column as target\n",
    "\n",
    "# Data scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Split the data into training and testing sets (70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# One-hot encode the target variable\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_encoded = to_categorical(y_train - 1, num_classes)\n",
    "y_test_encoded = to_categorical(y_test - 1, num_classes)\n",
    "\n",
    "# Early stopping callback setup\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Build a DNN model with L2 regularization\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.02), input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.02)),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Setup Adam optimizer\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_encoded,\n",
    "    epochs=50,\n",
    "    batch_size=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = model.evaluate(X_train_scaled, y_train_encoded, verbose=0)\n",
    "print(f'Training Accuracy: {train_accuracy}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_encoded, verbose=0)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Visualization of training loss and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWbnCzweyXi6"
   },
   "source": [
    "**Accuracy Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtwdaW6LscLf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# File Load\n",
    "file_path = '/content/drive/MyDrive/combined output.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop(['Frame Num', 'Class'], axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Data scaling for KNN and Deep Learning\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# KNN Model (k=10)\n",
    "k = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Random Forest Model\n",
    "rf_classifier = RandomForestClassifier(max_depth=16, random_state=42)\n",
    "\n",
    "# Build DNN model function\n",
    "def create_dnn_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.02), input_shape=(input_shape,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.02)),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Encode labels for DNN\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_encoded = to_categorical(y_train - 1, num_classes)\n",
    "y_test_encoded = to_categorical(y_test - 1, num_classes)\n",
    "\n",
    "# Evaluate KNN Model with Cross-Validation\n",
    "k_fold = 10\n",
    "knn_cv_scores = cross_val_score(knn, X_train_scaled, y_train, cv=k_fold)\n",
    "\n",
    "# Evaluate Random Forest Model with Cross-Validation\n",
    "rf_cv_scores = cross_val_score(rf_classifier, X_train, y_train, cv=k_fold)\n",
    "\n",
    "# Evaluate DNN Model with Cross-Validation\n",
    "kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "dnn_cv_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    X_train_cv, X_val_cv = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_cv, y_val_cv = y_train_encoded[train_index], y_train_encoded[val_index]\n",
    "    model = create_dnn_model(X_train_scaled.shape[1], num_classes)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=10, verbose=0, validation_data=(X_val_cv, y_val_cv), callbacks=[early_stopping])\n",
    "    score = model.evaluate(X_val_cv, y_val_cv, verbose=0)\n",
    "    dnn_cv_scores.append(score[1])\n",
    "\n",
    "dnn_cv_scores = np.array(dnn_cv_scores)\n",
    "\n",
    "# Calculate mean and standard deviation for KNN\n",
    "knn_mean_cv_accuracy = np.mean(knn_cv_scores)\n",
    "knn_std_cv_accuracy = np.std(knn_cv_scores)\n",
    "\n",
    "# Calculate mean and standard deviation for Random Forest\n",
    "rf_mean_cv_accuracy = np.mean(rf_cv_scores)\n",
    "rf_std_cv_accuracy = np.std(rf_cv_scores)\n",
    "\n",
    "# Calculate mean and standard deviation for DNN\n",
    "dnn_mean_cv_accuracy = np.mean(dnn_cv_scores)\n",
    "dnn_std_cv_accuracy = np.std(dnn_cv_scores)\n",
    "\n",
    "print(f'KNN Mean Cross-Validation Accuracy: {knn_mean_cv_accuracy}')\n",
    "print(f'KNN Standard Deviation of Cross-Validation Accuracy: {knn_std_cv_accuracy}')\n",
    "print(f'Random Forest Mean Cross-Validation Accuracy: {rf_mean_cv_accuracy}')\n",
    "print(f'Random Forest Standard Deviation of Cross-Validation Accuracy: {rf_std_cv_accuracy}')\n",
    "print(f'DNN Mean Cross-Validation Accuracy: {dnn_mean_cv_accuracy}')\n",
    "print(f'DNN Standard Deviation of Cross-Validation Accuracy: {dnn_std_cv_accuracy}')\n",
    "\n",
    "# Visualization of accuracy with error bars for all models\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(x=['KNN'], y=[knn_mean_cv_accuracy], yerr=[knn_std_cv_accuracy], fmt='o', capsize=5, capthick=2, label='KNN')\n",
    "plt.errorbar(x=['Random Forest'], y=[rf_mean_cv_accuracy], yerr=[rf_std_cv_accuracy], fmt='o', capsize=5, capthick=2, label='Random Forest')\n",
    "plt.errorbar(x=['DNN'], y=[dnn_mean_cv_accuracy], yerr=[dnn_std_cv_accuracy], fmt='o', capsize=5, capthick=2, label='DNN')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy with Error Bars')\n",
    "plt.legend()\n",
    "plt.ylim(0.99, 1.0)  # Adjust y-axis range as needed\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
